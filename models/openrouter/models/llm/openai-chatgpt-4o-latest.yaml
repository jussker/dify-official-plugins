model: openai/chatgpt-4o-latest
label:
  en_US: chatgpt-4o-latest
  zh_Hans: chatgpt-4o-latest
model_type: llm
features:
- vision
model_properties:
  mode: chat
  context_size: 128000
parameter_rules:
- name: temperature
  use_template: temperature
- name: top_p
  use_template: top_p
- name: presence_penalty
  use_template: presence_penalty
- name: frequency_penalty
  use_template: frequency_penalty
- name: max_completion_tokens
  use_template: max_tokens
  default: 5376
  min: 1
  max: 16384
- name: seed
  label:
    zh_Hans: Seed
    en_US: Seed
  type: int
  required: false
  help:
    zh_Hans: 指定后将尽可能使采样具有确定性；同一 seed 与参数应产生相同结果（部分模型不保证）。
    en_US: If specified, sampling will be as deterministic as possible; same seed
      and parameters should return the same result for some models.
- name: logit_bias
  label:
    zh_Hans: Logit 偏置
    en_US: Logit Bias
  type: string
  required: false
  help:
    zh_Hans: JSON 对象：tokenId -> 偏置（-100 到 100）。用于在采样前调整 logits。
    en_US: JSON object mapping token IDs to bias (-100 to 100). Applied to logits
      prior to sampling.
- name: logprobs
  label:
    zh_Hans: 返回对数概率
    en_US: Logprobs
  type: boolean
  required: false
  default: false
  help:
    zh_Hans: 是否返回输出 token 的对数概率。
    en_US: Whether to return log probabilities of output tokens.
- name: top_logprobs
  label:
    zh_Hans: Top 对数概率数量
    en_US: Top logprobs
  type: int
  required: false
  min: 0
  max: 20
  help:
    zh_Hans: 每个位置返回的最可能 token 数量（需开启 logprobs）。
    en_US: Number of most likely tokens to return at each position (requires logprobs).
- name: response_format
  use_template: response_format
  label:
    zh_Hans: 回复格式
    en_US: Response Format
  type: string
  required: false
  options:
  - text
  - json_object
  - json_schema
  help:
    zh_Hans: '强制输出特定格式。例如 {type: text|json_object|json_schema}。当选择 json_schema 时，请提供下方
      JSON Schema。'
    en_US: 'Force specific output format, e.g. {type: text|json_object|json_schema}.
      When using json_schema, also provide the JSON Schema below.'
- name: json_schema
  use_template: json_schema
- name: enable_stream
  label:
    zh_Hans: 启用流式输出
    en_US: Enable Stream
  type: boolean
  required: false
  default: true
  help:
    zh_Hans: 是否以流式方式返回模型输出。关闭后将一次性返回完整响应。
    en_US: Whether to stream model outputs. When disabled, a single complete response
      is returned.
- name: enable_provider
  label:
    zh_Hans: 启用 Provider 路由
    en_US: Enable Provider Routing
  type: boolean
  required: false
  default: false
  help:
    zh_Hans: 默认关闭。开启后可在“provider”对象中配置 OpenRouter Provider 路由偏好（order、only、ignore、allow_fallbacks、require_parameters、data_collection、quantizations、sort、max_price
      等）
    en_US: Disabled by default. When enabled, you can configure OpenRouter provider
      preferences via the 'provider' object (order, only, ignore, allow_fallbacks,
      require_parameters, data_collection, quantizations, sort, max_price, etc).
- name: provider
  label:
    zh_Hans: Provider 路由配置
    en_US: Provider Preferences
  type: text
  required: false
  help:
    zh_Hans: 仅当启用“启用 Provider 路由”后生效。请输入 JSON 字符串，系统会在调用前解析为对象并按 OpenRouter 规范透传，例如
      {order:["anthropic","openai"], allow_fallbacks:true, sort:"price", only:[],
      ignore:[], require_parameters:false, data_collection:"allow|deny", quantizations:["fp16"],
      max_price:{prompt:1, completion:2}}。
    en_US: Effective only when 'Enable Provider Routing' is on. Enter a JSON string;
      it will be parsed into an object before request and passed per OpenRouter spec,
      e.g. {order:["anthropic","openai"], allow_fallbacks:true, sort:"price", only:[],
      ignore:[], require_parameters:false, data_collection:"allow|deny", quantizations:["fp16"],
      max_price:{prompt:1, completion:2}}. See openrouter.ai/docs/features/provider-routing
pricing:
  input: '5.0'
  output: '15.0'
  unit: '0.000001'
  currency: USD
